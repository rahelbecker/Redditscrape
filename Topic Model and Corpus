setwd("C:/Users/Daniel/Mein Zeug/UNI/2. Semester Mannheim FSS 16/Big Data/Reddit/Analysis/Dta pure")
#source("C:/Users/Daniel/Mein Zeug/UNI/Arbeit/EPL FSQCA/Code/Utilities.R")
#setwd("C:/Users/Daniel/SkyDrive/Mein Zeug/UNI/2. Semester Mannheim FSS 16/Big Data/Reddit/Analysis/Dta pure")
#setwd("L:/HIWIs/2h Daniel/BD/Analysis/Dta pure")

library(tm)
library(RWeka)
library(plyr)
library(SnowballC)
library(topicmodels)
library(ggplot2)
library(ggrepel)
library(Hmisc)
library(foreach)
library(doParallel)

rr_u<-readRDS("rr_u_070515.rds")
source("C:/Users/Daniel/Mein Zeug/UNI/2. Semester Mannheim FSS 16/Big Data/Reddit/Analysis/TM utilities.R")
#source("L:/HIWIs/2h Daniel/BD/Analysis/TM u<tilities.R")
ss1prep<-subset(rr_u,select = c("selftext"))
rownames(ss1prep)<-(rr_u$id)

# lapply(ss1[1:2],meta)
# lapply(ss1[1:2],content)
#assign unique identifier to the texts

changemeta<-function(ss1,ss12){

  for(i in seq(ss1)){
  meta(ss1[[i]],"id")<-ss12[i,"id"]
  }
  return(ss1)
}

#myDebug(changemeta)



# General statistics ------------------------------------------------------

summary(rr_u$author)
#unique authors
length(unique(rr_u$author))

# Text prep ---------------------------------------------------------------
library(parallel)

stime<-system.time({
cl<-makeCluster(3)
clusterExport(cl,"ss1prep")

ss1prep$selftext<-parSapply(cl,ss1prep,function(x){
  #x<-gsub("\\[https?:[^\\]]*\\]", "",x, perl=T)
  #x<-gsub("\\(https?:[^\\]]*\\)", "",x, perl=T)
  x<-gsub("https?:[^\\]]*", "",x, perl=T)
  x <- gsub("/", " ", x)
  x <- gsub("\\|", " ", x)
  x <- gsub("([\\])", " ", x)
  x <- gsub("([\\\"])", " ", x)
  x <- gsub("\n", " ", x)
  x <- gsub("\n\n", " ", x)
  x <- gsub("\n\n&amp;nbsp;\n\n", " ", x)
  x <- gsub(",", " ", x)
  x <- gsub("'", " ", x)
  x <- gsub("nbsp", " ", x)
  x <- gsub( "[^[:alnum:]]", " ", x)
  return(x)
})
stopCluster(cl)
})
stime[3]


ss1<-VCorpus(DataframeSource(ss1prep))
ss1<-changemeta(ss1,rr_u)


#removing Punctuation
ss1<-tm_map(ss1,removePunctuation)

#removing numbers
ss1<-tm_map(ss1,removeNumbers)

#converting to lowercase
ss1<-tm_map(ss1,content_transformer(tolower))

ss1<-tm_map(ss1,stripWhitespace)

# #tokenization
# ss1<-tm_map(ss1,content_transformer(MC_tokenizer))

#removing stopwords
#ss1<-tm_map(ss1, removeWords,mystops)

#stopword list
stoplist<-c("yeah","ive","just","one","www","https","amp")
mystops<-c(stopwords("SMART"),stoplist)

ss1 <- tm_map(ss1, removeWords,mystops)


ss2<-ss1
#####Stemming
ss1<-tm_map(ss1,stemDocument)

#remove extra whitespace
ss1<-tm_map(ss1,stripWhitespace)

ss2<-tm_map(ss2,content_transformer(WordTokenizer))
#lapply(ss1[10:100],as.character)
#lapply(ss2[10:100],as.character)
ss1[[1]]


# Analysis ----------------------------------------------------------------

dtm<-DocumentTermMatrix(ss1)
# tdm<-TermDocumentMatrix(ss1)

#extract the term-frequencies(or other measure) from the dtm
freq<-colSums(as.matrix(dtm))
ord<-order(freq,decreasing=TRUE)
freq[head(ord,n=20L)]
freq[tail(ord,n=20L)]

#create matrix from the term-freq list
wf=data.frame(term=names(freq),occurrences=freq)

dtmr<-weightTfIdf(dtm,normalize = TRUE)

fr_dtmr<-colSums(as.matrix(dtmr))
fr_ord<-order(fr_dtmr,decreasing=TRUE)
fr_dtmr[head(fr_ord,n=20L)]
fr_dtmr[tail(fr_ord,n=20L)]

wf_dtmr<-data.frame(term=names(fr_dtmr),occurrences=fr_dtmr)


# findFreqTerms(tdm, 50)
#findAssocs(dtmr,"depress",0.2)

ggplot(wf_dtmr, aes(x=occurrences))+xlim(0,5)+
  geom_vline(xintercept=0.25)+geom_histogram(aes(y=..density..),binwidth = 0.025,
                                             fill="white",colour="black")+
  geom_density(alpha=0.2,fill="red")

length(wf_dtmr[wf_dtmr$occurrences<0.1,"occurrences"])

# Frequency bars ----------------------------------------------------------
chartwords(wf,freqcut=2000)


#wordclouds
library(wordcloud)
#setting the same seed each time ensures consistent look across clouds
set.seed(42)
#limit words by specifying min frequency
wordcloud(names(freq),freq, min.freq=1000)

wordcloud(names(fr_dtmr),fr_dtmr, min.freq=50)


# Topic Modelling ---------------------------------------------------------
#cleaning data for Gibbs sampling (i.e. removing 0 rows and noisy words)
library(slam)
#calculating average TFidF for each term
term_tfidf <-
  + tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *
  + log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)

tf<-as.data.frame(term_tfidf)
summary(tf$term_tfidf)
ggplot(tf, aes(x=term_tfidf))+xlim(0,2)+
  geom_vline(xintercept=c(median(tf$term_tfidf),mean(tf$term_tfidf)))+geom_histogram(aes(y=..density..),binwidth = 0.025,
                                             fill="white",colour="black")+
  geom_density(alpha=0.2,fill="red")

dtmlda<-dtm[,term_tfidf>=0.1]
dtmlda<-dtm[row_sums(dtm) > 0,]


#ldaOut<-LDA(dtmlda,k, method="Gibbs", control=mycontrol,.progress="text")

# #writing out output
#ldaOut.topics<-as.matrix(topics(ldaOut))
# ldaOut.terms<-as.matrix(terms(ldaOut,10))
# topicProbabilities <- as.data.frame(ldaOut@gamma)
#
#
# tprop<-termweigths(ldaOut,10)
#
#
# ggplot(data=tprop, aes(y=W1,x=seq(1,nrow(tprop))))+
#   geom_line(size=1,colour="green")+geom_point()+geom_text_repel(size=8,aes(label=tprop[,"Topic.1"]))
#
# ldaplot.m(tprop)

# Going Parallel ----------------------------------------------------------
library(topicmodels)
#Set parameters for Gibbs sampling
burnin <- 1000
iter <- 1000
thin <- 100
#seed <-list(2003,5,63,100001,765)
#nstart <- 5
seed <-list(2003)
nstart <- 1
best <- TRUE

#number of topics
k=5

mycontrol=list(nstart=nstart,
               seed = seed, best=best,
               burnin = burnin, iter = iter,
               thin=thin)

n_cores<-detectCores()-1

stimelda<-system.time({
cl <- makeCluster(n_cores)
rm(results_list)
registerDoParallel(cl)
results_list<-list()
clusterExport(cl,c("dtmlda","mycontrol"))
results_list<- foreach::foreach(i=seq(3,5,1),
                        .packages = c('topicmodels')) %dopar% {
                          result <- LDA(dtmlda,
                                k=i,method="Gibbs" ,control = mycontrol,.progress="text")}
stopCluster(cl)
})
stimelda[3]

#output for the 5 topic's model
ldaOut.topics<-as.matrix(topics(results_list[[3]]))
topicProbabilities <- data.frame(documents=results_list[[3]]@documents,results_list[[3]]@gamma)
topicProbabilities[order(topicProbabilities$X5,decreasing = T),][1,]

# Plotting ----------------------------------------------------------------

ptrop<-foreach(i=1:3) %do%{
  termweigths(results_list[[i]],10,results_list[[i]]@k)
}

ldaplot.m(ptrop[[1]],models = ncol(ptrop[[1]])/2)
ldaplot.m(ptrop[[2]],models = ncol(ptrop[[2]])/2)
ldaplot.m(ptrop[[3]],models = ncol(ptrop[[3]])/2)

# #access the text
# content(ss1[["4els3z"]])
# rr_u[rr_u$id=="4els3z","selftext"]
